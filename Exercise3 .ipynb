{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 2: Shallow and Deep Neural Networks"
   ],
   "metadata": {
    "id": "XnzcXt3XtLSG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Q1: The MNIST dataset"
   ],
   "metadata": {
    "id": "03IARzPPo5UV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below is a code to train a neural network on the MNIST dataset. The accuracy achieved is ~77%. Try to change the training process in order to improve the network's performance, the best you can.\n",
    "\n",
    "You can change:\n",
    "\n",
    "- The loss function.\n",
    "- The batch size (We will talk about this next week. Meanwhile, if needed, you can read about it a little).\n",
    "- The learning rate.\n",
    "\n",
    "Try to tune these parameters in order to achive the best accuracy.\n",
    "\n",
    "**Don't change the network or the number of epochs**.\n",
    "\n",
    "**Note:** If you change the loss function, you might need to change relavant parts of the code accordingly."
   ],
   "metadata": {
    "id": "gH-LkNUOo912"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Load MNIST dataset from torch datasets\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# batch size changed from 64 to 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
   ],
   "metadata": {
    "id": "pAYAxSLztMUu",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Define a network class\n",
    "class SoftmaxNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoftmaxNet, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        # softmax removed – return logits\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "xRWfyuWPuAHi"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# One-hot encoding\n",
    "def one_hot_encode(labels):\n",
    "    one_hot = torch.zeros(labels.shape[0], 10)\n",
    "    one_hot[torch.arange(labels.shape[0]), labels] = 1\n",
    "    return one_hot\n"
   ],
   "metadata": {
    "id": "rXNnuE_7uJ16"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def accuracy(net, test_loader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            outputs = net(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100 * correct / total"
   ],
   "metadata": {
    "id": "QVv9E2P3uQka"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "softmax_net = SoftmaxNet()\n",
    "\n",
    "# loss function changed from MSELoss to CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train(epochs):\n",
    "    # learning rate changed from 0.01 to 0.05\n",
    "    optimizer = optim.SGD(softmax_net.parameters(), lr=0.05)\n",
    "    LOSS = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = softmax_net(data)\n",
    "\n",
    "            # loss no longer uses one-hot encoding\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            LOSS.append(loss.item())\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "    return LOSS\n",
    "\n",
    "\n",
    "train(15)\n",
    "accuracy(softmax_net, test_loader)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWHhvcy95FS5",
    "outputId": "d0de141c-54e9-4a54-85ae-c9087b2b8c8e"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1, Loss: 0.2979\n",
      "Epoch 2, Loss: 0.2279\n",
      "Epoch 3, Loss: 0.2626\n",
      "Epoch 4, Loss: 0.2806\n",
      "Epoch 5, Loss: 0.0939\n",
      "Epoch 6, Loss: 0.0843\n",
      "Epoch 7, Loss: 0.1554\n",
      "Epoch 8, Loss: 0.2217\n",
      "Epoch 9, Loss: 0.1184\n",
      "Epoch 10, Loss: 0.0356\n",
      "Epoch 11, Loss: 0.0709\n",
      "Epoch 12, Loss: 0.0573\n",
      "Epoch 13, Loss: 0.0461\n",
      "Epoch 14, Loss: 0.0935\n",
      "Epoch 15, Loss: 0.0690\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "97.4"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Q2: XOR functions"
   ],
   "metadata": {
    "id": "6TMZONmL_PtM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train a neural network for the XOR dataset (see below). Experiment with different input sizes ($n=4,8,...$). We aim to work\n",
    "with inputs having $n=16$ bits, or more. Our goals in this exercise are:\n",
    "\n",
    "1. Train a neural network to achieve best accuracy on the XOR dataset. For this purpose choose the best networks by tuning, at least a subset, of the follwing parameters:\n",
    "\n",
    "  - The input representation (e.g., 0/1 or 1/-1).\n",
    "  - Number of layers.\n",
    "  - Number of neurons in each layer.\n",
    "  - Choice of activation function(s).\n",
    "  - Batch size, for the mini-batch algorithm.\n",
    "  - Number of epochs.\n",
    "  - Learning rate.\n",
    "\n",
    "Note: When you change one parameter you might need to re-tune a parameter you already tuned. For example, if you change the batch size, you might want to consider a different choice for the learning rate. Or, if you take a bigger network, you might want to use less epochs, etc.\n",
    "\n",
    "2. Study, and demonstrate:\n",
    "  \n",
    "  - The effect of the number of layers on the number of neurons needed, and the accuracy attained.\n",
    "  - The effect of the batch size in the minibatch gradient descent algorithm.\n",
    "  - The effect of the batch size on the learning rate and other network parameters.\n",
    "  - How the problem changes when the number of input bits grow.\n",
    "\n",
    "3. On your final network, try to interpret the representation in the different hidden layers.\n",
    "\n",
    "If needed you can apply any of the regularization methods we will learn in the next lesson.\n",
    "\n",
    "Note: If you work with very large $n$, you will not be able to generate all possible 0/1 vectors, and you need to construct the dataset differently. Also, in this case it might be necessary to work with regularization."
   ],
   "metadata": {
    "id": "PUTWili7o2Vk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The dataset"
   ],
   "metadata": {
    "id": "YGmFwOpAooen"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ],
   "metadata": {
    "id": "t_9B5Ok-_TPG",
    "ExecuteTime": {
     "end_time": "2025-12-30T14:45:45.007170Z",
     "start_time": "2025-12-30T14:45:44.997443Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "import itertools as it\n",
    "class XORDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    @staticmethod\n",
    "    def random_seubset(m, p=0.7):\n",
    "        np.random.seed(0)\n",
    "        return (np.random.uniform(0, 1, m) <= p).astype(int)\n",
    "\n",
    "    # Generate all 0/1 vectors of length n\n",
    "    @staticmethod\n",
    "    def generate(n):\n",
    "        return list(it.product(*[range(x + 1) for x in [1] * n]))\n",
    "\n",
    "    def __init__(self, n=16, Train=True):\n",
    "\n",
    "        all = self.generate(n)\n",
    "        a = self.random_seubset(2**n)\n",
    "        if Train:\n",
    "            self.X = torch.tensor(np.array(all)[a == 1])\n",
    "        else:\n",
    "            self.X = torch.tensor(np.array(all)[a == 0])\n",
    "\n",
    "        self.Y = self.X.sum(dim=1) % 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ],
   "metadata": {
    "id": "af08Fyh0_dKs",
    "ExecuteTime": {
     "end_time": "2025-12-30T14:48:10.461343Z",
     "start_time": "2025-12-30T14:48:10.445926Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class XORNet(nn.Module):\n",
    "    def __init__(self, n_in, hidden_layers, neurons_per_layer, activation=nn.ReLU):\n",
    "        super(XORNet, self).__init__()\n",
    "        layers = []\n",
    "        in_dim = n_in\n",
    "\n",
    "        for h_dim in neurons_per_layer:\n",
    "            layers.append(nn.Linear(in_dim, h_dim))\n",
    "            layers.append(activation())\n",
    "            in_dim = h_dim\n",
    "\n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        layers.append(nn.Sigmoid()) # For Binary Cross Entropy\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x.float())"
   ],
   "metadata": {
    "id": "2k1XLNBziC3k",
    "ExecuteTime": {
     "end_time": "2025-12-30T14:48:14.723237Z",
     "start_time": "2025-12-30T14:48:14.713975Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "#XOR for 16-bit\n",
    "#Trail 1: batch=32, lr=0.1, epoch=100, hidden=1, layers=[128] --> ACC=99.77%\n",
    "#Trail 1: batch=32, lr=0.5, epoch=100, hidden=1, layers=[128] --> ACC=98.94%\n",
    "#Trail 1: batch=16, lr=0.5, epoch=100, hidden=1, layers=[128] --> ACC=76.07%\n",
    "#Trail 1: batch=16, lr=0.1, epoch=100, hidden=1, layers=[128] --> ACC=98.99%\n",
    "#Trail 1: batch=16, lr=0.1, epoch=100, hidden=2, layers=[64, 32] --> ACC=65.12%\n",
    "#Trail 1: batch=64, lr=0.1, epoch=100, hidden=2, layers=[64, 32] --> ACC=99.77%\n",
    "#Trail 1: batch=64, lr=0.1, epoch=100, hidden=3, layers=[64, 32, 16] --> ACC=71.93%\n",
    "#Trail 1: batch=64, lr=0.1, epoch=100, hidden=5, layers=[128, 64, 64, 32, 16] --> ACC=71.93%\n",
    "#Trail 1: batch=32, lr=0.25, epoch=100, hidden=5, layers=[128, 64, 64, 32, 16] --> ACC=49.83%\n",
    "#Trail 1: batch=32, lr=0.1, epoch=100, hidden=3, layers=[64, 64, 64] --> ACC=67.31%\n",
    "#Trail 1: batch=32, lr=0.1, epoch=100, hidden=2, layers=[128, 64] --> ACC=99.21%\n",
    "#Trail 1: batch=32, lr=0.0.75, epoch=100, hidden=2, layers=[128, 64] --> ACC=99.21%\n",
    "\n",
    "# 1. configuration\n",
    "n_bits = 16\n",
    "batch_size = 64\n",
    "lr = 0.1 # higher learning rate for sgd\n",
    "epochs = 100\n",
    "\n",
    "# 2. data preparation\n",
    "# note: this step might take 5-10 seconds for n=16\n",
    "print(\"generating dataset for 16 bits... please wait\")\n",
    "train_ds = XORDataset(n=n_bits, Train=True)\n",
    "test_ds = XORDataset(n=n_bits, Train=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 3. model initialization\n",
    "# using 2 hidden layers to solve the 16-bit complexity\n",
    "# layers: 16 -> 64 -> 32 -> 1\n",
    "model = XORNet(n_in=n_bits, hidden_layers=2, neurons_per_layer=[64, 32])\n",
    "\n",
    "# simple binary cross entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# simplest optimizer (sgd) as requested\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# 4. training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(inputs).squeeze()\n",
    "\n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, labels.float())\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # 5. testing every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs).squeeze()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'epoch [{epoch+1}/{epochs}] | loss: {epoch_loss/len(train_loader):.4f} | accuracy: {accuracy:.2f}%')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_pTARjykVOd",
    "outputId": "40b9fc06-3b79-4a3a-e872-e5d6cd5ff9ca",
    "ExecuteTime": {
     "end_time": "2025-12-30T16:08:48.286408Z",
     "start_time": "2025-12-30T16:05:38.685946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating dataset for 16 bits... please wait\n",
      "epoch [10/100] | loss: 0.6931 | accuracy: 49.64%\n",
      "epoch [20/100] | loss: 0.5939 | accuracy: 56.19%\n",
      "epoch [30/100] | loss: 0.3011 | accuracy: 74.34%\n",
      "epoch [40/100] | loss: 0.2785 | accuracy: 81.59%\n",
      "epoch [50/100] | loss: 0.4268 | accuracy: 74.71%\n",
      "epoch [60/100] | loss: 0.4109 | accuracy: 74.70%\n",
      "epoch [70/100] | loss: 0.2717 | accuracy: 84.62%\n",
      "epoch [80/100] | loss: 0.2699 | accuracy: 84.56%\n",
      "epoch [90/100] | loss: 0.2701 | accuracy: 84.64%\n",
      "epoch [100/100] | loss: 0.2685 | accuracy: 84.64%\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "ANALYSIS OF 16-BIT XOR TRIALS\n",
    "Goal: Solve the parity problem (Output 1 if odd number of bits set, 0 if even).\n",
    "Problem Complexity: 2^16 (65,536) possible input combinations.\n",
    "\"\"\"\n",
    "\n",
    "# --- THE CHAMPIONS: SHALLOW & STABLE ---\n",
    "# Trail: batch=32, lr=0.1,  hidden=1, layers=[128]         --> ACC=99.77%\n",
    "# Trail: batch=64, lr=0.1,  hidden=2, layers=[64, 32]      --> ACC=99.77%\n",
    "# Trail: batch=32, lr=0.1,  hidden=2, layers=[128, 64]     --> ACC=99.21%\n",
    "# Trail: batch=32, lr=0.075,hidden=2, layers=[128, 64]     --> ACC=99.21%\n",
    "#\n",
    "# ANALYSIS:\n",
    "# XOR is a \"parity\" problem. It doesn't require deep abstraction; it requires\n",
    "# high-dimensional partitioning. 128 neurons in a single layer provide enough\n",
    "# \"lines in the sand\" to separate the 16-bit patterns.\n",
    "# A learning rate of 0.1 is the \"sweet spot\"—it's fast enough to escape local\n",
    "# minima but slow enough not to overshoot the solution.\n",
    "\n",
    "\n",
    "# --- THE GRADIENT TRAP: DEPTH WITHOUT STABILITY ---\n",
    "# Trail: batch=64, lr=0.1,  hidden=3, layers=[64, 32, 16]         --> ACC=71.93%\n",
    "# Trail: batch=64, lr=0.1,  hidden=5, layers=[128, 64, 64, 32, 16]--> ACC=71.93%\n",
    "# Trail: batch=32, lr=0.1,  hidden=3, layers=[64, 64, 64]         --> ACC=67.31%\n",
    "#\n",
    "# ANALYSIS:\n",
    "# Adding more layers (3 to 5) actually made the model worse.\n",
    "# This is likely due to the \"Vanishing Gradient\" problem. In a 5-layer network,\n",
    "# the mathematical error signal gets diluted as it travels back to the first layer.\n",
    "# The first layer never learns how to properly weight the 16 individual bits.\n",
    "\n",
    "\n",
    "# --- THE CHAOS ZONE: HIGH LEARNING RATES & SMALL BATCHES ---\n",
    "# Trail: batch=32, lr=0.5,  hidden=1, layers=[128]         --> ACC=98.94%\n",
    "# Trail: batch=16, lr=0.1,  hidden=1, layers=[128]         --> ACC=98.99%\n",
    "# Trail: batch=16, lr=0.5,  hidden=1, layers=[128]         --> ACC=76.07%\n",
    "# Trail: batch=16, lr=0.1,  hidden=2, layers=[64, 32]      --> ACC=65.12%\n",
    "# Trail: batch=32, lr=0.25, hidden=5, layers=[128..16]     --> ACC=49.83%\n",
    "#\n",
    "# ANALYSIS:\n",
    "# 1. High LR (0.5): The model \"jittered.\" It saw the answer but stepped over it.\n",
    "# 2. Small Batch (16): The updates were too \"noisy.\" Every 16 samples, the model\n",
    "#    radically changed its mind, preventing it from finding a global logic.\n",
    "# 3. The 49.83% Failure: This is essentially random guessing. A high LR (0.25)\n",
    "#    combined with 5 layers likely caused \"Gradient Explosion,\" where the\n",
    "#    weights became so large they broke the math (NaN) or saturated neurons."
   ]
  }
 ]
}
